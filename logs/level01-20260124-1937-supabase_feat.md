# Log: Level 01 - Supabase Integration (Complete)

**Date (KST)**: 2026-01-24 19:37
**Commit Hash**: `661f7f5` (Initial), `3cd0952` (Log Update)

## 1. Summary
We successfully migrated the data storage from local files to Supabase Cloud DB (PostgreSQL). This log serves as the main documentation for Level 1.

## 2. Modified Files (Direct Links)
- **Supabase Storage Logic**: [crawler/storage_supabase.py](../crawler/storage_supabase.py)
- **Main Crawler**: [crawler/main.py](../crawler/main.py)
- **Environment Config**: [crawler/.env](../crawler/.env)
- **SQL Schema**: [crawler/schema.sql](../crawler/schema.sql)

## 3. Learning Guide & Concepts

### A. Environment Variables (`.env`)
instead of hardcoding API keys in Python code (which is dangerous), we use a `.env` file.
```python
# python-dotenv matches entries in .env to os.environ
from dotenv import load_dotenv
import os

load_dotenv()
url = os.getenv("SUPABASE_URL")
key = os.getenv("SUPABASE_KEY")
```

### B. Supabase & PostgreSQL
**Supabase** is an open-source Firebase alternative based on **PostgreSQL**.
We used **SQL** to create structured tables.

**SQL Used (Schema):**
```sql
-- Table for Subway Data
create table if not exists subway_traffic (
  id bigint generated by default as identity primary key,
  date date not null,
  station_name text not null,
  line_number text not null,
  boarding_count int,
  alighting_count int,
  unique(date, station_name, line_number) -- Prevent duplicates
);
```

### C. The `Upsert` Operation
We used `upsert` instead of simple `insert`.
- **Insert**: Fails if data already exists (due to unique constraint).
- **Upsert**: "Update if exists, Insert if new". Perfect for daily data collection scripts running repeatedly.

```python
# 'on_conflict' tells Supabase which columns define uniqueness
client.table("subway_traffic").upsert(
    data, 
    on_conflict="date, station_name, line_number"
).execute()
```

### D. Supabase-py Library
We use the official Python client to talk to the DB without writing raw SQL inside Python strings.
```python
from supabase import create_client
client = create_client(url, key)
response = client.table("weather_data").insert(weather_json).execute()
```

## 4. Execution History
1.  **API Verification**: Verified Seoul Data & KMA keys were working.
2.  **Supabase Auth**: Fixed issue where the short Project ID was used instead of the long JWT `service_role` key.
3.  **Table Creation**: User manually ran the SQL schema in Supabase Dashboard.
4.  **Connection Test**: Confirmed successful Write access to the Cloud DB.

## 5. Next Steps (Level 2)
Now that data is flowing to the cloud, Level 2 will focus on:
- Reading data from Supabase into Python (Pandas).
- Preprocessing (Cleaning, Feature Engineering).
- Preparing datasets for AI training.
